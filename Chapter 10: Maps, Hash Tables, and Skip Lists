
---------------------------10.1.2 Application: Counting Word Frequencies ------------------------

problem of counting the number of occurrences of words in a document. using dictionary

#see the code

---------------------------- 10.1.3 Python’s MutableMapping Abstract Base Class ----------------------------------

The collections module provides two abstract base classes that are relevant to
our current discussion: the Mapping and MutableMapping classes. The Mapping
class includes all nonmutating methods supported by Python’s dict class, while the
MutableMapping class extends that to include the mutating methods. What we
define as the map ADT in Section 10.1.1 is akin to the MutableMapping abstract
base class in Python’s collections module.

In particular, the MutableMapping class provides concrete implementations for all behaviors other than the first five outlined in 
Section 10.1.1: __getitem__ , __setitem__ , __delitem__ , __len__ , and __iter__ . As we implement the map abstraction with various data structures, as
long as we provide the five core behaviors, we can inherit all other derived behaviors by simply declaring MutableMapping as a parent 
class.
  
To better understand the MutableMapping class, we provide a few examples of
how concrete behaviors can be derived from the five core abstractions. For example,
the contains method, supporting the syntax k in M, could be implemented by
making a guarded attempt to retrieve self[k] to determine if the key exists.
def __contains__(self, k):
  try:
    self[k] # access via getitem (ignore result)
    return True
  except KeyError:
    return False # attempt failed
    
A similar approach might be used to provide the logic of the setdefault method.
def setdefault(self, k, d):
  try:
    return self[k] # if getitem succeeds, return value
  except KeyError: # otherwise:
    self[k] = d # set default value with setitem
    return d # and return that newly assigned value


---------------------- 10.1.4 Our MapBase Class ------------------------------

The MutableMapping abstract base class, from Python’s collections module
and discussed in the preceding pages, is a valuable tool when implementing a map.
However, in the interest of greater code reuse, we define our own MapBase class,
which is itself a subclass of the MutableMapping class.

                                      MutableMapping ( Collections Module)
                                            !
             _____________________________MapBase________________________________
             !                  !                        !                      !
        UnsortedTableMap    HashMapBase              SortedTableMap          TreeMap
                          ______!_______
                         !              !
                   ChainHashMap    ProbeHashMap
                   
#see the code of MapBase

-------------------------------------- 10.1.5 Simple Unsorted Map Implementation -----------------------------------------

We demonstrate the use of the MapBase class with a very simple concrete implementation of the map ADT.
An implementation of a map using a Python list as an unsorted table.

#see the code



 10.2 Hash Tables 
                   
we introduce one of the most practical data structures for implementing a map, and the one that is used by Python’s own implementation 
of the dict class. This structure is known as a hash table.

in hash tables we store the value associated with key k at index k of the table (presuming that we have a distinct way to represent an 
empty slot). Basic map operations of getitem , setitem , and delitem can be implemented in O(1) worst-case time.


There are two challenges in extending this framework to the more general setting of a map:
  1.we may not wish to devote an array of length N if it is the case that N >> n. 
  2.we do not in general require that a map’s keys be integers.

The novel concept for a hash table is the use of a hash function to map general keys to corresponding indices in a table. 
Ideally, keys will be well distributed in the range from 0 to N −1 by a hash function, but in practice there may be two or more
distinct keys that get mapped to the same index. As a result, we will conceptualize our table as a bucket array(at one index one or
more key value pair are getting stored).


----------------------------------------------- 10.2.1 Hash Functions ----------------------------------------------------------

The goal of a hash function, h, is to map each key k to an integer in the range
[0,N −1], where N is the capacity of the bucket array for a hash table. Equipped
with such a hash function, h, the main idea of this approach is to use the hash
function value, h(k), as an index into our bucket array, A, instead of the key k
(which may not be appropriate for direct use as an index). That is, we store the
item (k,v) in the bucket A[h(k)].

If there are two or more keys with the same hash value, then two different items
will be mapped to the same bucket in A. In this case, we say that a collision has
occurred.

there are ways to deal with the collisions but best strategy is to avoid them.

It is common to view the evaluation of a hash function, h(k), as consisting of two portions—a hash code that maps a key k to an integer,
and a compression function that maps the hash code to an integer within a range of indices, [0,N −1].
                                     
                                     Arbitrary Objects
                                            ||
                                        hash Code
                                            ||
                              ..........-2,-1,0,1,2........
                                            ||
                                       Compression Function
                                            ||
                                    0, 1...................N-1


by breaking hash function into two part, now we can use the hash code for any size of hash table, and only compression function have to
deal with the size not the hash code

-------------------------------------------Hash Codes------------------------------------------------

-------------------Treating the Bit Representation as an Integer:

For any data type X that is represented using at most as many
bits as our integer hash codes, we can simply take as a hash code for X an integer interpretation of its bits. 
For example, the hash code for key 314 could simply be 314. The hash code for a floating-point number such as 3.14 could be based upon
an interpretation of the bits of the floating-point representation as an integer.

For a type whose bit representation is longer than a desired hash code, the above scheme is not immediately applicable. 
For example, Python relies on 32-bit hash codes. If a floating-point number uses a 64-bit representation, its bits cannot be
viewed directly as a hash code. One possibility is to use only the high-order 32 bits (or the low-order 32 bits). 
This hash code, of course, ignores half of the information present in the original key, and if many of the keys in our map only differ 
in these bits, then they will collide using this simple hash code.
A better approach is to combine in some way the high-order and low-order portions of a 64-bit key to form a 32-bit hash code, which 
takes all the original bits into consideration. A simple implementation is to add the two components as 32-bit numbers 
(ignoring overflow), or to take the exclusive-or of the two components. These approaches of combining components can be extended to 
any object x whose binary representation can be viewed as an n-tuple (x0,x1, . . . ,xn−1) of 32-bit integers,
for example, by forming a hash code for x as Σn−1 i=0 xi, or as x0⊕x1 ⊕···⊕xn−1,
where the ⊕ symbol represents the bitwise exclusive-or operation (which is ˆ in Python).

-------------------------------Polynomial Hash Codes: 

just adding the unicode value of the character of the string will producce lot of coliision(above approch). like "temp01" and
"temp10" collide using this function, as do "stop", "tops", "pots", and "spot". 
A better hash code should somehow take into consideration the positions of the char in the string. 
An alternative hash code, which does exactly this,  is to choose a nonzero constant, a != 1, and use as a hash code the value
x0an−1 +x1an−2+· · ·+xn−2a+xn−1.

#see the code

--------------------------------Cyclic-Shift Hash Codes: 
A variant of the polynomial hash code replaces multiplication by a with a cyclic shift of a partial sum by a 
certain number of bits. For example, a 5-bit cyclic shift of the 32-bit value 00111101100101101010100010101000 is achieved by taking
the leftmost five bits and placing those on the rightmost side of the representation, resulting in 10110010110101010001010100000111.

#see the ccode

the shift amount should be taken appropriatly to reduce the collision


---------------------------------------------- Hash Codes in Python: -------------------------------------------

The standard mechanism for computing hash codes in Python is a built-in function with signature hash(x) that returns an integer value 
that serves as the hash code for object x. 
However, only immutable data types are deemed hashable in Python. This restriction is meant to ensure that a particular object’s hash 
code remains constant during that object’s lifespan.

Hash codes for character strings are well crafted based on a technique similar to polynomial hash codes,
except using exclusive-or computations rather than additions.

Hash codes for tuples are computed with a similar technique based upon a combination of the 
hash codes of the individual elements of the tuple

When hashing a frozenset, the order of the elements should be irrelevant, and so a natural option is to compute the
exclusive-or of the individual hash codes without any shifting.

If hash(x) is called for an instance x of a mutable type, such as a list, a TypeError is raised.

Instances of user-defined classes are treated as unhashable by default, with a TypeError raised by the hash function. 
However, a function that computes hash codes can be implemented in the form of a special method named hash within
a class. The returned hash code should reflect the immutable attributes of an instance. 

def hash (self):
  return hash( (self. red, self. green, self. blue) ) # hash combined tuple

An important rule to obey is that if a class defines equivalence through __eq__ ,then any implementation of hash must be consistent, 
in that if x == y, then hash(x) == hash(y).

For example, since Python treats the expression 5 == 5.0 as true, it ensures that hash(5) and hash(5.0) are the same.



----------------------------------------------- Compression Functions --------------------------------------------------------------

these functions will compress the hash values generated by the hash code within the range [0,N−1].

----------The Division Method:

A simple compression function is the division method, which maps an integer i to

i mod N

N: size of the bucket array, is a fixed positive integer.

Additionally, if we take N to be a prime number, then this compression function helps “spread out” the distribution of hashed values. 
Indeed, if N is not prime, then there is greater risk that patterns in the distribution of hash codes will be repeated in the 
distribution of hash values, thereby causing collisions.
Choosing N to be a prime number is not always enough, however, for if there is a repeated pattern of hash codes of the form pN +q for 
several different p’s, then there will still be collisions.

If a hash function is chosen well, it should ensure that the probability of two different keys getting hashed to the same bucket is 1/N.

---------------The MAD Method: Multiply-Add-and-Divide

This method maps an integer i to
[(ai+b) mod p] mod N

N : size of the bucket array
p : prime number larger than N 
a,b : integers chosen at random from the interval [0, p−1], with a > 0.

This compression function is chosen in order to eliminate repeated patterns in the set of hash codes and get us closer to having a 
“good” hash function, that is, one such that the probability any two different keys collide is 1/N. This good behavior would be
the same as we would have if these keys were “thrown” into A uniformly at random.


------------------------------------------------ 10.2.2 Collision-Handling Schemes ------------------------------------------------

------------Sperate Chaining:

In this approch each bucket A[j] store its own secondary container, A natural choice for the secondary container is a small map instance
implemented using a list. (each position of the bucket array will have it's own list or linked lit or map to store multipe values)

load factor = n/N 
n: total no of elements in our hash table
N: size of the bucket array

the best O(1) when our hash function distribute all the key equally
worst case O(n), when hash function generate same hash value for all the n keys


---------------Open Addressing:

The separate chaining rule has many nice properties, such as affording simple implementations of map operations, but it nevertheless has
one slight disadvantage:
It requires the use of an auxiliary data structure—a list—to hold items with colliding keys. If space is at a premium (for example, 
if we are writing a program for a small handheld device), then we can use the alternative approach of always storing
each item directly in a table slot.

Open addressing approach saves space because no auxiliary structures are employed, but it requires a bit more complexity to deal with collisions.

Open addressing requires that the load factor is always at most 1 and that items are stored directly in the cells of the bucket
array itself.

Below are some methods which are used to avoid collision in open addressing:

1. Linear Probing: In this if for some key collision occure we check the next position until we find empty spot. in case of removing
                   we can't simply remove we need to set a flag after removing the key.

With this approach, if we try to insert an item (k,v) into a bucket A[ j] that is already occupied, where j = h(k), then we next try 
A[( j+1) mod N]. If A[( j+1) mod N] is also occupied, then we try A[( j+2) mod N], and so on, until we find an empty
bucket that can accept the new item.
while with this approach, to attempt to locate an item with key equal to k, we must examine consecutive slots, starting from A[h(k)], 
until we either find an item with that key or we find an empty bucket.

To implement a deletion, we cannot simply remove a found item from its slot in the array. typical way to get around this difficulty
is to replace a deleted item with a special “available” marker object. With this special marker possibly occupying spaces in our hash 
table, we modify our search algorithm so that the search for a key k will skip over cells containing the available marker and continue 
probing until reaching the desired item or an empty bucket.

Additionally, our algorithm for setitem should remember an available cell encountered during the search for k, 
since this is a valid place to put a new item (k,v), if no existing item is found.

disadvantage of linear probing is that, It tends to cluster the items of a map into contiguous runs, which may even overlap(particularly
if more than half of the cells in the hash table are occupied). Such contiguous runs of occupied hash cells cause searches to slow down 
considerably.
                   
2. Quadratic Probing: In this we do not move to next position rather we move to the quadratic positions like hash_value + 1**2 then
                      hash_value + 2**2 and so on.
Another open addressing strategy, known as quadratic probing, iteratively tries the buckets A[(h(k)+ f (i)) mod N], for i =0,1,2, . . .,
where f (i) =i**2, until finding an empty bucket.

this approach complicates the removal operation, though this probing remove the linear probing clustring patern but it create it's own 
clustering know as secondary clustering.
                     
3. Double Hashing: In this we use 2 hash fucntions. when collision occurs we use the second hash function to generate the value and 
                   then add up that value with the previous hash function's output and that's how we get our new address.
                   
                   2nd hash function should always produce different output from the 1st one for same key
                   
                   2nd function should not producce 0 as we are adding up the two values

An open addressing strategy that does not cause clustering of the kind produced by linear probing or the kind produced by quadratic 
probing is the double hashing strategy. In this approach, we choose a secondary hash function, h', and if h maps
some key k to a bucket A[h(k)] that is already occupied, then we iteratively try the buckets A[(h(k)+ f (i)) mod N] next, 
for i = 1,2,3, . . ., where f (i) = i · h'(k).
In this scheme, the secondary hash function is not allowed to evaluate to zero; a common choice is h'(k) = q−(k mod q), for some prime 
number q < N. Also, N should be a prime.

4. Another approach to avoid clustering with open addressing is to iteratively try buckets A[(h(k)+ f (i)) mod N] where f (i) is based 
on a pseudo-random number generator, providing a repeatable, but somewhat arbitrary, sequence of subsequent probes that depends upon
bits of the original hash code. This is the approach currently used by Python’s dictionary class.


--------------------------------------------- 10.2.3 Load Factors, Rehashing, and Efficiency -------------------------------------

--------- Load Factor:

In the hash table schemes described thus far, it is important that the load factor, λ = n/N, be kept below 1.

Separate chaining: as λ gets very close to 1, the probability of a collision greatly increases, which adds overhead to our operations,
                   since we must revert to linear-time list-based methods in buckets that have collisions. 
                   experiments suggest to have λ < 0.9 for hash tables with separate chaining.

Open addressing: as the load factor λ grows beyond 0.5 and starts approaching 1, clusters of entries in the bucket array start to grow 
                 as well. These clusters cause the probing strategies to “bounce around” the bucket array for a considerable amount of 
                 time before they find an empty slot.
                 Experiments suggest that we should maintain λ < 0.5 for an open addressing scheme with linear probing, and perhaps only
                 a bit higher for other open addressing schemes.
                 for example, Python’s implementation of open addressing enforces that λ < 2/3

------------------Rehashing:
            If an insertion causes the load factor of a hash table to go above the specified threshold, then it is common to resize the 
           table (to regain the specified load factor) and to reinsert all objects into this new table. Although we need not define a
           new hash code for each object, we do need to reapply a new compression function that takes into consideration the size of the
           new table.
           When rehashing to a new table, it is a good requirement for the new array’s size to be at least double the previous size.
           (amortize the cost of rehashing)

--------------Efficiency of Hash Tables: 

If our hash function is good, then we expect the entries to be uniformly distributed in the N cells of the bucket
array. Thus, to store n entries, the expected number of keys in a bucket would
be [n/N], which is O(1) if n is O(N).

In the worst case, a poor hash function could map every item to the same bucket. This would result in linear-time performance for the 
core map operations


Operation       List      Hash Table
                      expected worst case
__getitem__     O(n)   O(1)     O(n)
__setitem__     O(n)   O(1)     O(n)
__delitem__     O(n)   O(1)     O(n)
__len__         O(1)   O(1)     O(1)
__iter__        O(n)   O(n)     O(n)

In practice, hash tables are among the most efficient means for implementing a map, and it is essentially taken for granted by 
programmers that their core operations run in constant time.

pyhton dict class is implemented withh the hashing.

Python interpreter relies on dictionaries to retrieve an object that is referenced by an identifier in a given namespace.

The basic command c = a + b involves two calls to __getitem__ in the dictionary for the local namespace to retrieve the values 
identified as a and b, and a call to __setitem__ to store the result associated with name c in that namespace.


-------------------------------------------------10.2.4 Python Hash Table Implementation -------------------------------------

The main design elements of the HashMapBase class are:
  • The bucket array is represented as a Python list, named self. table, with all entries initialized to None.
  • We maintain an instance variable self. n that represents the number of distinct items that are currently stored in the hash table.
  • If the load factor of the table increases beyond 0.5, we double the size of the table and rehash all items into the new table.
  • We define a hash function utility method that relies on Python’s built-in hash function to produce hash codes for keys, and a     
 randomized Multiply-Add-and-Divide (MAD) formula for the compression function

# See the code HashMapBase.py

------------------------------------------------ 10.3 Sorted Maps --------------------------------------------------------------

sorted map ADT that includes all behaviors of the standard map, plus the following:

M.find_min()             : Return the (key,value) pair with minimum key (or None, if map is empty).
M.find_max()             : Return the (key,value) pair with maximum keyv(or None, if map is empty).
M.find_lt(k)             : Return the (key,value) pair with the greatest key thatvis strictly less than k (or None, if no such item 
                           exists).
M.find_le(k)             : Return the (key,value) pair with the greatest key that is less than or equal to k (or None, if no such item
                           exists).
M.find_gt(k)             : Return the (key,value) pair with the least key that is strictly greater than k (or None, if no such item
                           exists).
M.find_ge(k)             : Return the (key,value) pair with the least key that is greater than or equal to k (or None, if no such item).
M.find_range(start, stop): Iterate all (key,value) pairs with start <= key < stop. If start is None, iteration begins with minimum key; 
                           if stop is None, iteration concludes with maximum key.
iter(M)                  : Iterate all keys of the map according to their natural order, from smallest to largest.
reversed(M)              : Iterate all keys of the map in reverse order; in Python, this is implemented with the __reversed__ method.


Time Analysis

Operation 					         Running Time
len(M)					           : O(1)
k in M 					           : O(logn)
M[k] = v 				           : O(n) worst case; O(logn) if existing k  # O(n) in the case when key dosen't exist and it needs to be
                                                                  # inserted at the first index in that case all the other
                                                                  # items needs to be shifted to next positions and hence needs n moves
del M[k] 				           : O(n) worst case                      # same as insertion if deletion take place at fisrt index
M.find_min(), M.find_max() : O(1)
M.find_lt(k), M.find_gt(k) : O(logn)
M.find_le(k), M.find_ge(k) : O(logn)
M.find_range(start, stop)  : O(s+logn) where s items are reported   # log(n) time to locate the first element of range
                                                                    # and then s time to traverse the s items
iter(M), reversed(M) 	     : O(n)


-----------------------------------------10.3.2 Two Applications of Sorted Maps ---------------------------------------------------


--------------------1 Flight Databases:
Internet that allow users to perform queries on flight databases to find flights between various cities, typically with the intent to
buy a ticket. To make a query, a user specifies origin and destination cities, a departure date, and a departure time. To support such
queries, we can model the flight database as a map, where keys are Flight objects that contain fields corresponding
to these four parameters. That is, a key is a tuple

k = (origin,destination,date,time)

--------------------2 Maxima Sets:



------------------------------------------------- 10.4 Skip Lists --------------------------------------------------------------

An interesting data structure for realizing the sorted map ADT is the skip list.

A skip list S for a map M consists of a series of lists {S0,S1, . . . ,Sh}. Each list Si stores a subset of the items of M sorted by 
increasing keys, plus items with two sentinel keys denoted −∞ and +∞,

in general, Si to have about n/2**i items, but it's not mandatory.
The halving of the number of items from one list to the next is not enforced as an explicit property of skip lists, however. 
Instead, randomization is used.

The main advantage of using randomization in data structure and algorithm design is that the structures and functions that result are
usually simple and efficient.

Functions that generate numbers that can be viewed as random numbers are built into most modern computers, because they are used 
extensively in computer games, cryptography, and computer simulations.
Some functions, called pseudorandom number generators, generate random-like numbers, starting with an initial seed. 
Other methods use hardware devices to extract “true” random numbers from nature.


The skip list has the same logarithmic time bounds for searching as is achieved by the binary search algorithm, but skip list have the
same bound for update methods when inserting or deleting items. 
Nevertheless, the bounds are expected for the skip list i.e. it is not exactly log(n) but maybe nearby, while binary search has a 
worst-case bound with a sorted table.

A skip list makes random choices in arranging its structure in such a way that search and update times are O(logn) on average, where n 
is the number of items in the map.

Using the position abstraction used for lists and trees, we view a skip list as a two-dimensional collection of positions arranged 
horizontally into levels and vertically into towers.

The positions in a skip list can be traversed using the following operations:

next(p): Return the position following p on the same level.
prev(p): Return the position preceding p on the same level.
below(p): Return the position below p in the same tower.
above(p): Return the position above p in the same tower.


maximum level = log(n) +1

------------------Searching in a Skip List:

We begin the SkipSearch method by setting a position variable p to the topmost, left position in the skip list S, called the start
position of S. That is, the start position is the position of Sh storing the special entry with key−∞.

1. If S.below(p) is None, then the search terminates—we are at the bottom and have located the item in S with the largest key less than 
or equal to the search key k. Otherwise, we drop down to the next lower level in the present tower by setting p = S.below(p).

2. Starting at position p, we move p forward until it is at the rightmost position on the present level such that key(p) ≤ k. We call 
this the scan forward step. Note that such a position always exists, since each level contains the keys +∞ and −∞. It may be that p 
remains where it started after we perform such a forward scan for this level.

3. Return to step 1.

where key(p) denotes the key of the item at position p:

Algo:

Algorithm SkipSearch(k):
  Input: A search key k
  Output: Position p in the bottom list S0 with the largest key such that key(p)≤k
  p = start {begin at start position}
  while below(p) != None do
    p = below(p) {drop down}
    while k ≥ key(next(p)) do
      p = next(p) {scan forward}
  return p.

Then above Algo will give us the position which is <= to our key, once we get the position we can chek the return position's key with our
searched key and if match give the position otherwise raise Key error

p = SkipSearch(k)
if key(p) == k:
  return p
else:
  raise KeyError("key is not present: ", repr(k))

----------------Insertion in a Skip List:

The execution of the map operation M[k] = v begins with a call to SkipSearch(k).
This gives us the position p of the bottom-level item with the largest key less than or
equal to k (note that p may hold the special item with key −∞). If key(p) = k, the
associated value is overwritten with v. Otherwise, we need to create a new tower for
item (k,v). We insert (k,v) immediately after position p within S0. After inserting
the new item at the bottom level, we use randomization to decide the height of the
tower for the new item. We “flip” a coin, and if the flip comes up tails, then we stop
here. Else (the flip comes up heads), we backtrack to the previous (next higher)
level and insert (k,v) in this level at the appropriate position. We again flip a coin;
if it comes up heads, we go to the next higher level and repeat. Thus, we continue
to insert the new item (k,v) in lists until we finally get a flip that comes up tails.

A coin flip can be simulated with Python’s built-in pseudo-random
number generator from the random module by calling randrange(2), which returns
0 or 1, each with probability 1/2.

Algorithm SkipInsert(k,v):
  Input: Key k and value v
  Output: Topmost position of the item inserted in the skip list
  p = SkipSearch(k)
  q = None                                   {q will represent top node in new item’s tower}
  i = −1
  repeat
    i = i+1
    if i ≥ h then
    h = h+1                                  {add a new level to the skip list}
    t = next(s)
    s = insertAfterAbove(None, s, (−∞,None)) {grow leftmost tower}
    insertAfterAbove(s, t,(+∞,None))         {grow rightmost tower}
    while above(p) is None do
    p = prev(p) {scan backward}
    p = above(p) {jump up to higher level}
    q = insertAfterAbove(p,q, (k,v))          {increase height of new item’s tower}
  until coinFlip() == tails
  n = n+1
  return q
  
  
--------------------Removal in a Skip List:

to perform the map operation del M[k] we begin by executing method SkipSearch(k).
If the position p stores an entry with key different from k, we raise a KeyError.
Otherwise, we remove p and all the positions above p, which are easily accessed
by using above operations to climb up the tower of this entry in S starting at position
p. While removing levels of the tower, we reestablish links between the horizontal
neighbors of each removed position.

we do not actually need to store references to values at the levels of the skip list above the bottom level,
because all that is needed at these levels are references to keys. In fact, we can
more efficiently represent a tower as a single object, storing the key-value pair,
and maintaining j previous references and j next references if the tower reaches
level Sj . 
Second, for the horizontal axes, it is possible to keep the list singly linked,
storing only the next references. We can perform insertions and removals in strictly
a top-down, scan-forward fashion.


---------------------Maintaining the Topmost Level:

A skip list S must maintain a reference to the start position (the topmost, left position
in S) as an instance variable, and must have a policy for any insertion that
wishes to continue inserting a new entry past the top level of S. There are two
possible courses of action we can take, both of which have their merits.

1. One possibility is to restrict the top level, h, to be kept at some fixed value that
is a function of n, the number of entries currently in the map (from the analysis we
will see that h=max{10,2log[n]} is a reasonable choice, and picking h=3log [n]
is even safer).

2. The other possibility is to let an insertion continue inserting a new position as
long as heads keeps getting returned from the random number generator.
we implemented our SkipInsert using this approach

Either choice will still result in the expected O(logn) time to perform search, insertion, and removal

------------------------------------------ 10.4.2 Probabilistic Analysis of Skip Lists ----------------------------:



In terms of worst-case performance, however, skip lists are not a superior data
structure. In fact, if we do not officially prevent an insertion from continuing significantly
past the current highest level, then the insertion algorithm can go into what
is almost an infinite loop (it is not actually an infinite loop, however, since the probability
of having a fair coin repeatedly come up heads forever is 0). Moreover, we
cannot infinitely add positions to a list without eventually running out of memory.
In any case, if we terminate position insertion at the highest level h, then the worstcase
running time for performing the getitem , setitem , and delitem
map operations in a skip list S with n entries and height h is O(n+h). This worstcase
performance occurs when the tower of every entry reaches level h−1, where
h is the height of S. However, this event has very low probability

------------------ Bounding the Height of a Skip List:

Because the insertion step involves randomization, a more accurate analysis of skip
lists involves a bit of probability.

Let us begin by determining the expected value of the height h of a skip list S
with n entries

The probability that a given entry has a tower of height i ≥ 1 is equal to the probability of getting i
consecutive heads when flipping a coin, that is, this probability is 1/2**i. Hence, the
probability Pi that level i has at least one position is at most
Pi ≤ n/2**i : for the probability that any one of n different events occurs is at most the sum of the probabilities that each occurs.

The probability that the height h of S is larger than i is equal to the probability
that level i has at least one position, that is, it is no more than Pi. This means that h
is larger than, say, the ith poition is 3logn, so the probability that atleast one entry will reach that level is:
P3logn ≤ n/2**3logn
        =n/n3 
        =1/n2
For example, if n = 1000, this probability is a one-in-a-million long shot.
More generally, given a constant c > 1, h is larger than clogn with probability at most
1/n**c−1. That is, the probability that h is smaller than clog n is at least 1−1/n**c−1.

this means that probability of having height nlog n is very less.
Thus, with high probability, the height h of S is O(logn).

---------------------- Analyzing Search Time in a Skip List:

Next, consider the running time of a search in skip list S, and recall that such a
search involves two nested while loops. The inner loop performs a scan forward on
a level of S as long as the next key is no greater than the search key k, and the outer
loop drops down to the next level and repeats the scan forward iteration.

Since the height h of S is O(logn) with high probability, the number of drop-down steps is
O(logn) with high probability.

Let ni be the number of keys examined while scanning forward at level i.

Observe that, after the key at the starting position, each additional key examined in a scan-forward at
level i cannot also belong to level i+1. If any of these keys were on the previous
level, we would have encountered them in the previous scan-forward step. Thus,
the probability that any key is counted in ni is 1/2. Therefore, the expected value of
ni is exactly equal to the expected number of times we must flip a fair coin before
it comes up heads. This expected value is 2. Hence, the expected amount of time
spent scanning forward at any level i is O(1). 

Since S has O(logn) levels with high probability, a search in S takes expected time O(log n).


By a similar analysis, we can show that the expected running time of an insertion or a removal is O(log n).

------------------------------ Space Usage in a Skip List:

As we observed above, the expected number of positions at level i is n/2**i.
So the expected total number of positions in S is:

h            h
Σ n/2**i = n Σ 1/2**i
i=0          i=0
n
Using the Geometric Sum:

= 2.(1 - 1/2**(h+1)) < 2 for all h>= 0

Hence, the expected space requirement of S is O(n).


Operation                     Running Time
len(M)                      : O(1)
k in M                      : O(logn) expected
M[k] = v                    : O(logn) expected
del M[k]                    : O(logn) expected
M.find_min(), M.find_max()  : O(1)
M.find_lt(k), M.find_gt(k)  : O(logn) expected
M.find_le(k), M.find_ge(k)  : O(logn) expected
M.find_range(start, stop)   : O(s+logn) expected, with s items reported
iter(M), reversed(M)        : O(n)

--------------------------------------------- 10.5 Sets, Multisets, and Multimaps -----------------------------


• A set is an unordered collection of elements, without duplicates, that typically supports efficient membership tests. In essence, 
elements of a set are like keys of a map, but without any auxiliary values.
"python sets are mutable but it can only accomodate non mutable type."

• A multiset (also known as a bag) is a set-like container that allows duplicates.

• A multimap is similar to a traditional map, in that it associates values with keys; however, in a multimap the same key can be mapped 
to multiple values.
For example, the index of this book maps a given term to one or more locations at which the term occurs elsewhere in the book.



------------------------ 10.5.1 The Set ADT -----------------------------

Python provides support for representing the mathematical notion of a set through the built-in classes frozenset and set,
with frozenset being an immutable form. Both of those classes are implemented using
hash tables in Python.

Python’s collections module defines abstract base classes that essentially mirror
these built-in classes.

The abstract base class collections.Set matches the concrete frozenset class,
while the abstract base class collections.MutableSet is akin to the concrete set class.

we equate the “set ADT” with the behavior of the builtin set class (and thus, the collections.MutableSet base class).
five most fundamental behaviors for a set S:

S.add(e)    : Add element e to the set. This has no effect if the set already contains e.
S.discard(e): Remove element e from the set, if present. This has no effect if the set does not contain e.
e in S      : Return True if the set contains element e. In Python, this is implemented with the special __contains__ method.
len(S)      : Return the number of elements in set S. In Python, this is implemented with the special method __len__ .
iter(S)     : Generate an iteration of all elements of the set. In Python, this is implemented with the special method __iter__ .

additional operations for removing one or more elements from a set:
S.remove(e) : Remove element e from the set. If the set does not contain e, raise a KeyError.
S.pop()     : Remove and return an arbitrary element from the set. If the set is empty, raise a KeyError.
S.clear()   : Remove all elements from the set.

S and T are two sets, below are the boolean bw the two:

S==T, S!=T : equal if contents are identical else not equal return tre and false
S<=T       : Return True if set S is a subset of set T.
S < T      : Return True if set S is a proper subset of set T.
S >= T     : Return True if set S is a superset of set T.
S > T      : Return True if set S is a proper superset of set T.
S.isdisjoint(T): Return True if sets S and T have no common elements.

S | T : Return a new set representing the union of sets S and T.
S |= T: Update set S to be the union of S and set T.
S & T : Return a new set representing the intersection of sets S and T.
S &= T: Update set S to be the intersection of S and set T.
S ˆ T : Return a new set representing the symmetric difference of sets S and T, that is, a set of elements that are in precisely one of
        S or T.
        exclude the common elements of both the sets
S ˆ= T: Update set S to become the symmetric difference of itself and set T.
S − T : Return a new set containing elements in S but not T.
S −= T: Update set S to remove all common elements with set T.

--------------------------- 10.5.2 Python’s MutableSet Abstract Base Class:

Python’s collections module provides a MutableSet abstract base class.

The MutableSet base class provides concrete implementations for all methods, except forfive core behaviors 
(add, discard, contains , len , and iter ) that must be implemented by any concrete subclass.

let's examine algorithms for implementing several of the derived methods of the MutableSet base 

Example 1: To determine if one set is a proper subset of another, we must verify two conditions: 
  1. a proper subset must have size strictly smaller than that of its superset
  2. each element of a subset must be contained in the superset.
An implementation of the corresponding __lt__ method based on this logic;

lt (self, other): # supports syntax S < T
  """Return true if this set is a proper subset of other"""
  if len(self) >= len(other):
    return False # proper subset must have strictly smaller size
  for e in self:
    if e not in other:
      return False # not a subset since element missing from other
  return True # success; all conditions are met

A possible implementation of the MutableSet. __lt__ method, which tests if one set is a proper subset of another

Example 2: Union of the two sets

The syntax S | T should produce a new set that has contents equal to the union of existing sets S and T. 
This operation is implemented through the special method __or__ in Python.

def or (self, other): # supports syntax S | T
  """Return a new set that is the union of two existing sets"""
  result = type(self)( ) # create new instance of concrete class
  for e in self:
    result.add(e)
  for e in other:
    result.add(e)

An implementation of the MutableSet.__or__ method, which computes the union of two existing sets.

When computing the union of two such concrete instances, the result should presumably be an instance of the same class as the
operands. The function type(self) returns a reference to the actual class of the instance identified as self, and the subsequent 
parentheses in expression type(self)( ) call the default constructor for that class.

efficiency:
n = size of set S
m = size of set T
then If the concrete sets are implemented with hashing, the expected running time of the implementation in Code Fragment 10.15 is
O(m+n), because it loops over both sets, performing constant-time operations in the form of a containment check and a possible insertion
into the result.


Another syntax, S |= T is used to update existing set S to become the union of itself and set T. Therefore, all elements of T that are 
not already contained in S should be added to S.

We note that this “in-place” operation may be implemented more efficiently than if we were to rely on the first form, using the syntax 
S = S | T, in which identifier S is reassigned to a new set instance that represents the union.

def __ior__(self, other):       # supports syntax S |= T
  """Modify this set to be the union of itself an another set"""
  for e in other:
    self.add(e)
  return self                   # technical requirement of in-place operator


The __ior__ special method that supports syntax S |= T. Notice that in this case, we do not create a new set instance, instead we modify
and return the existing set, after updating its contents to reflect the union operation. The in-place version of the union has expected 
running time O(m) where m is the size of the second set, because we only have to loop through that second set.

----------------------------------------------- 10.5.3 Implementing Sets, Multisets, and Multimaps ------------------------------------

Sets:
Although sets and maps have very different public interfaces, they are really quite similar. A set is simply a map in which keys do not 
have associated values. Any data structure used to implement a map can be modified to implement the set ADT with similar performance 
guarantees. We could trivially adapt any map class by storing set elements as keys, and using None as an irrelevant value, but such an 
implementation is unnecessarily wasteful. An efficient set implementation should abandon the Item composite that we use in our MapBase
class and instead store set elements directly in a data structure.

Multisets:
The same element may occur several times in a multiset. All of the data structures we have seen can be reimplemented to allow for
duplicates to appear as separate elements. However, another way to implement a multiset is by using a map in which the map key is a 
(distinct) element of the multiset, and the associated value is a count of the number of occurrences of that element within the multiset

Python’s standard collections module includes a definition for a class named Counter that is in essence a multiset. Formally, the 
Counter class is a subclass of dict, with the expectation that values are integers, and with additional functionality

The standard iter reports each element only once (since those are formally the keys of the dictionary). There is another method named
elements( ) that iterates through the multiset with each element being repeated according to its count.

Multimaps:

Although there is no multimap in Python’s standard libraries, a common implementation approach is to use a standard map in which the 
value associated with a key is itself a container class storing any number of associated values.

#see the code in book for imlementation
