
---------------------------10.1.2 Application: Counting Word Frequencies ------------------------

problem of counting the number of occurrences of words in a document. using dictionary

#see the code

---------------------------- 10.1.3 Python’s MutableMapping Abstract Base Class ----------------------------------

The collections module provides two abstract base classes that are relevant to
our current discussion: the Mapping and MutableMapping classes. The Mapping
class includes all nonmutating methods supported by Python’s dict class, while the
MutableMapping class extends that to include the mutating methods. What we
define as the map ADT in Section 10.1.1 is akin to the MutableMapping abstract
base class in Python’s collections module.

In particular, the MutableMapping
class provides concrete implementations for all behaviors other than the first five
outlined in Section 10.1.1: __getitem__ , __setitem__ , __delitem__ , __len__ , and
__iter__ . As we implement the map abstraction with various data structures, as
long as we provide the five core behaviors, we can inherit all other derived behaviors
by simply declaring MutableMapping as a parent class.
  
To better understand the MutableMapping class, we provide a few examples of
how concrete behaviors can be derived from the five core abstractions. For example,
the contains method, supporting the syntax k in M, could be implemented by
making a guarded attempt to retrieve self[k] to determine if the key exists.
def __contains__(self, k):
  try:
    self[k] # access via getitem (ignore result)
    return True
  except KeyError:
    return False # attempt failed
    
A similar approach might be used to provide the logic of the setdefault method.
def setdefault(self, k, d):
  try:
    return self[k] # if getitem succeeds, return value
  except KeyError: # otherwise:
    self[k] = d # set default value with setitem
    return d # and return that newly assigned value


---------------------- 10.1.4 Our MapBase Class ------------------------------

The MutableMapping abstract base class, from Python’s collections module
and discussed in the preceding pages, is a valuable tool when implementing a map.
However, in the interest of greater code reuse, we define our own MapBase class,
which is itself a subclass of the MutableMapping class.

                                      MutableMapping ( Collections Module)
                                            !
             _____________________________MapBase________________________________
             !                  !                        !                      !
        UnsortedTableMap    HashMapBase              SortedTableMap          TreeMap
                          ______!_______
                         !              !
                   ChainHashMap    ProbeHashMap
                   
#see the code of MapBase

-------------------------------------- 10.1.5 Simple Unsorted Map Implementation -----------------------------------------

We demonstrate the use of the MapBase class with a very simple concrete implementation of the map ADT.
An implementation of a map using a Python list as an unsorted table.

#see the code



 10.2 Hash Tables 
                   
we introduce one of the most practical data structures for implementing a map, and the one that is used by Python’s own implementation 
of the dict class. This structure is known as a hash table.

in hash tables we store the value associated with key k at index k of the table (presuming that we have a distinct way to represent an 
empty slot). Basic map operations of getitem , setitem , and delitem can be implemented in O(1) worst-case time.


There are two challenges in extending this framework to the more general setting of a map:
  1.we may not wish to devote an array of length N if it is the case that N >> n. 
  2.we do not in general require that a map’s keys be integers.

The novel concept for a hash table is the use of a hash function to map general keys to corresponding indices in a table. 
Ideally, keys will be well distributed in the range from 0 to N −1 by a hash function, but in practice there may be two or more
distinct keys that get mapped to the same index. As a result, we will conceptualize our table as a bucket array(at one index one or
more key value pair are getting stored).


----------------------------------------------- 10.2.1 Hash Functions ----------------------------------------------------------

The goal of a hash function, h, is to map each key k to an integer in the range
[0,N −1], where N is the capacity of the bucket array for a hash table. Equipped
with such a hash function, h, the main idea of this approach is to use the hash
function value, h(k), as an index into our bucket array, A, instead of the key k
(which may not be appropriate for direct use as an index). That is, we store the
item (k,v) in the bucket A[h(k)].

If there are two or more keys with the same hash value, then two different items
will be mapped to the same bucket in A. In this case, we say that a collision has
occurred.

there are ways to deal with the collisions but best strategy is to avoid them.

It is common to view the evaluation of a hash function, h(k), as consisting of two portions—a hash code that maps a key k to an integer,
and a compression function that maps the hash code to an integer within a range of indices, [0,N −1].
                                     
                                     Arbitrary Objects
                                            ||
                                        hash Code
                                            ||
                              ..........-2,-1,0,1,2........
                                            ||
                                       Compression Function
                                            ||
                                    0, 1...................N-1


by breaking hash function into two part, now we can use the hash code for any size of hash table, and only compression function have to
deal with the size not the hash code

-------------------------------------------Hash Codes------------------------------------------------

Treating the Bit Representation as an Integer:

For any data type X that is represented using at most as many
bits as our integer hash codes, we can simply take as a hash code for X an integer interpretation of its bits. 
For example, the hash code for key 314 could simply be 314. The hash code for a floating-point number such as 3.14 could be based upon
an interpretation of the bits of the floating-point representation as an integer.

For a type whose bit representation is longer than a desired hash code, the above scheme is not immediately applicable. 
For example, Python relies on 32-bit hash codes. If a floating-point number uses a 64-bit representation, its bits cannot be
viewed directly as a hash code. One possibility is to use only the high-order 32 bits (or the low-order 32 bits). 
This hash code, of course, ignores half of the information present in the original key, and if many of the keys in our map only differ 
in these bits, then they will collide using this simple hash code.
A better approach is to combine in some way the high-order and low-order portions of a 64-bit key to form a 32-bit hash code, which 
takes all the original bits into consideration. A simple implementation is to add the two components as 32-bit numbers 
(ignoring overflow), or to take the exclusive-or of the two components. These approaches of combining components can be extended to 
any object x whose binary representation can be viewed as an n-tuple (x0,x1, . . . ,xn−1) of 32-bit integers,
for example, by forming a hash code for x as Σn−1 i=0 xi, or as x0⊕x1 ⊕···⊕xn−1,
where the ⊕ symbol represents the bitwise exclusive-or operation (which is ˆ in Python).

Polynomial Hash Codes: 

just adding the unicode value of the character of the string will producce lot of coliision(above approch). like "temp01" and
"temp10" collide using this function, as do "stop", "tops", "pots", and "spot". 
A better hash code should somehow take into consideration the positions of the char in the string. 
An alternative hash code, which does exactly this,  is to choose a nonzero constant, a != 1, and use as a hash code the value
x0an−1 +x1an−2+· · ·+xn−2a+xn−1.

#see the code

Cyclic-Shift Hash Codes: 
A variant of the polynomial hash code replaces multiplication by a with a cyclic shift of a partial sum by a 
certain number of bits. For example, a 5-bit cyclic shift of the 32-bit value 00111101100101101010100010101000 is achieved by taking
the leftmost five bits and placing those on the rightmost side of the representation, resulting in 10110010110101010001010100000111.

#see the ccode

the shift amount should be taken appropriatly to reduce the collision


----------------------------Hash Codes in Python: ---------------------------------

The standard mechanism for computing hash codes in Python is a built-in function with signature hash(x) that returns an integer value 
that serves as the hash code for object x. 
However, only immutable data types are deemed hashable in Python. This restriction is meant to ensure that a particular object’s hash 
code remains constant during that object’s lifespan.

Hash codes for character strings are well crafted based on a technique similar to polynomial hash codes,
except using exclusive-or computations rather than additions.

Hash codes for tuples are computed with a similar technique based upon a combination of the 
hash codes of the individual elements of the tuple

When hashing a frozenset, the order of the elements should be irrelevant, and so a natural option is to compute the
exclusive-or of the individual hash codes without any shifting.

If hash(x) is called for an instance x of a mutable type, such as a list, a TypeError is raised.

Instances of user-defined classes are treated as unhashable by default, with a TypeError raised by the hash function. 
However, a function that computes hash codes can be implemented in the form of a special method named hash within
a class. The returned hash code should reflect the immutable attributes of an instance. 

def hash (self):
  return hash( (self. red, self. green, self. blue) ) # hash combined tuple

An important rule to obey is that if a class defines equivalence through __eq__ ,then any implementation of hash must be consistent, 
in that if x == y, then hash(x) == hash(y).

For example, since Python treats the expression 5 == 5.0 as true, it ensures that hash(5) and hash(5.0) are the same.



----------------------------------------------- Compression Functions --------------------------------------------------------------

these functions will compress the hash values generated by the hash code within the range [0,N−1].

----------The Division Method:

A simple compression function is the division method, which maps an integer i to

i mod N

N: size of the bucket array, is a fixed positive integer.

Additionally, if we take N to be a prime number, then this compression function helps “spread out” the distribution of hashed values. 
Indeed, if N is not prime, then there is greater risk that patterns in the distribution of hash codes will be repeated in the 
distribution of hash values, thereby causing collisions.
Choosing N to be a prime number is not always enough, however, for if there is a repeated pattern of hash codes of the form pN +q for 
several different p’s, then there will still be collisions.

If a hash function is chosen well, it should ensure that the probability of two different keys getting hashed to the same bucket is 1/N.

---------------The MAD Method: Multiply-Add-and-Divide

This method maps an integer i to
[(ai+b) mod p] mod N

N : size of the bucket array
p : prime number larger than N 
a,b : integers chosen at random from the interval [0, p−1], with a > 0.

This compression function is chosen in order to eliminate repeated patterns in the set of hash codes and get us closer to having a 
“good” hash function, that is, one such that the probability any two different keys collide is 1/N. This good behavior would be
the same as we would have if these keys were “thrown” into A uniformly at random.

------------------------------------------------ 10.2.2 Collision-Handling Schemes ------------------------------------------------


------------Sperate Chaining:

In this approch each bucket A[j] store its own secondary container, A natural choice for the secondary container is a small map instance
implemented using a list. (each position of the bucket array will have it's own list or linked lit or map to store multipe values)

load factor = n/N 
n: total no of elements in our hash table
N: size of the bucket array

the best O(1) when our hash function distribute all the key equally
worst case O(n), whene hash function generate same hash value for all the n keys


---------------Open Addressing:
The separate chaining rule has many nice properties, such as affording simple implementations of map operations, but it nevertheless has
one slight disadvantage:
It requires the use of an auxiliary data structure—a list—to hold items with colliding keys. If space is at a premium (for example, 
if we are writing a program for a small handheld device), then we can use the alternative approach of always storing
each item directly in a table slot.

This approach saves space because no auxiliary structures are employed, but it requires a bit more complexity to deal with collisions.

Open addressing requires that the load factor is always at most 1 and that items are stored directly in the cells of the bucket
array itself.

Below are some methods which are used to avoid collision in open addressing:

1. Linear Probing: In this if for some key collision occure we check the next position until we find empty spot. in case of removing
                   we can't simply remove we need to set a flag after removing the key.
                   
2. Quadratic Probing: In this we do not move to next position rather we move to the quadratic positions like hash_value + 1**2 then
                      hash_value + 2**2 and so on.
                     
3. Double Hashing: In this we use 2 hash fucntions. when collision occurs we use the second hash function to generate the value and 
                   then add up that value with the previous hash function's output and that's how we get our new address
                   
                   2nd hash function should always produce different output from the 1st one for same key
                   
                   2nd function should not producce 0 as we are adding up the two values











